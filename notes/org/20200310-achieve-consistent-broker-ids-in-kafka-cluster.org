#+title: Achieve Consistent Broker IDs in Kafka Cluster
# #+setupfile: ~/Dropbox/git/org-html-themes/setup/theme-readtheorg.setup
#+setuptitle: https://github.com/fniessen/org-html-themes/tree/master/setup/theme-readtheorg.setup

Have you ever found the broker ids are changed after a rebuild of Kafka cluster? What is worse, the brokers cannot be added into consumer group since their ids are different from last time. This note provide a solution for such issue. The goal is to assign exact broker id during deployment so that they are always consistent after rebuild or recover from crash.

To help the discussion, we are going to assume a Kafka cluster deployed using Kafka and Zookeeper Helm charts from [[https://github.com/bitnami/charts/tree/master/bitnami/kafka][this repository]] provided by Bitnami.

* The Problem
  :PROPERTIES:
  :ID:       116c3d98-9817-4444-98e7-36e6f7ea554d
  :END:
The Chart use a ~statefulset~ to deploy both [[https://github.com/bitnami/charts/blob/master/bitnami/kafka/templates/statefulset.yaml][Kafka]] and [[https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/templates/statefulset.yaml][Zookeeper]]. [[https://github.com/bitnami/charts/blob/master/bitnami/kafka/values.yaml][values.yaml]] files are used to specify the cluster configurations. The broker id is provided as ~brokerId=-1~, which means broker ids will be auto-generated. According to the [[https://kafka.apache.org/documentation/][Apache Kafka]]:
#+begin_quote
*~broker.id~*: The broker id for this server. If unset, a unique broker id will be generated.To avoid conflicts between zookeeper generated broker id's and user configured broker id's, generated broker ids start from ~reserved.broker.max.id~ + 1.
#+end_quote
The ~reserved.broker.max.id~ defaults to 1000. Ideally, auto-generated broker ids should start form 1001. For a 3 node cluster, the broker id list is [1001,1002,1003]. However, during one restart of the such Kafka cluster, there happened an issue that Kafka client cannot read any message from the existing topics. After digging around for a while, it becomes clear that root cause is that the brokers are not accepted into the consumer group due to changes of broker ids after restart.
This is uncovered by comparing the metadata of ~__consumer_offsets~ topic which records all offset committed by consumer groups and the broker ids of after restart.
The topic metadata look like this. Notice that it suggests the replicas of the topic are in broker 1001,1002,1003 and the Leader is missing.
#+name: consumer_offsets_metadata
#+begin_src shell
$ /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka-cluster.kafka:9092 --describe --topic __consumer_offsets

Topic: __consumer_offsets	PartitionCount: 50	ReplicationFactor: 3
Configs: compression.type=producer,cleanup.policy=compact,flush.ms=1000,
segment.bytes=104857600,flush.messages=10000,max.message.bytes=1000012,retention.bytes=1073741824
	Topic: __consumer_offsets	Partition: 0	Leader: none	Replicas: 1002,1003,1001	Isr: 1003
	Topic: __consumer_offsets	Partition: 1	Leader: none	Replicas: 1003,1001,1002	Isr: 1003
	Topic: __consumer_offsets	Partition: 2	Leader: none	Replicas: 1001,1002,1003	Isr: 1003
	Topic: __consumer_offsets	Partition: 3	Leader: none	Replicas: 1002,1001,1003	Isr: 1003
	Topic: __consumer_offsets	Partition: 4	Leader: none	Replicas: 1003,1002,1001	Isr: 1003
	Topic: __consumer_offsets	Partition: 5	Leader: none	Replicas: 1001,1003,1002	Isr: 1003
	Topic: __consumer_offsets	Partition: 6	Leader: none	Replicas: 1002,1003,1001	Isr: 1003
	Topic: __consumer_offsets	Partition: 7	Leader: none	Replicas: 1003,1001,1002	Isr: 1003
	Topic: __consumer_offsets	Partition: 8	Leader: none	Replicas: 1001,1002,1003	Isr: 1003
	Topic: __consumer_offsets	Partition: 9	Leader: none	Replicas: 1002,1001,1003	Isr: 1003
	Topic: __consumer_offsets	Partition: 10	Leader: none	Replicas: 1003,1002,1001	Isr: 1003
    ...
#+end_src
However, the broker ids for the new cluster are:
#+name: get_brokder_ids
#+begin_src shell
$ /opt/bitnami/zookeeper/bin/zkCli.sh ls /brokers/ids
[1004, 1005, 1006]
#+end_src
Clearly, the ~__consumer_offsets~ topic is not able to recognized the brokers by their new ids; thus rejecting them from the consume group. The ~__consumer_offsets~ topic along with other topic that actually contain the messages in Kafka are saved a Persistent Volumes, which is in persistent state during restart to provide durability of Kafka service. It seems that the ~reserved.broker.max.id~ is not stateless during restart (meaning it is not reset to 1000). This is why we get the new broker ids as [1004, 1005, 1006] and they are not able to join the consumer group. As a result, the Kafka client cannot read from any topic.


* The Solution
  :PROPERTIES:
  :ID:       6137230e-5751-4b9b-9fc4-b9387406938f
  :END:
How can we solve this issue given that we cannot delete Persistent Volumes during cluster rebuild? One potential way it so assign a id to aforementioned ~brokerId~ directly. But this only works for a cluster with single node because each node in the cluster has to have a unique broker id. One potential solution explored here is to tie the broker id with ~MY_POD_NAME~ in the given [[https://github.com/bitnami/charts/blob/master/bitnami/kafka/templates/statefulset.yaml][statefulset]] template. Concretely, assume the Helm release name is ~Kafka-cluster~, ~MY_POD_NAME~ will be ~kafka-cluster-{0,1,2}~. The number [0,1,2] can be assigned as broker id to for the broker that host corresponding POD. Since ~MY_POD_NAME~ is deterministic for each restart or rebuilt, we are able to determine broker ids without ambiguity. The solution is submitted as a [[https://github.com/bitnami/charts/pull/2028][Pull Request]] to Bitnami's Charts repository. The key code snippet added to [[https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/templates/statefulset.yaml][statefulset.yaml]] in the ~kafka~ container session is:
#+name: pr-consistent-broker-id
#+begin_src yaml
          command: ["/bin/bash", "-c"]
          args:
            - export KAFKA_CFG_BROKER_ID=${MY_POD_NAME##*-};
              exec /entrypoint.sh /run.sh;
#+end_src
This code snippet overwrites the ~ENTRYPOINT~ and ~CMD~ inside the Kafka docker image (the [[https://hub.docker.com/layers/bitnami/kafka/2.4.0-debian-10-r44/images/sha256-ce5ed55be807573b5902c5995982b346a01fe9f2cb77d2d2efe26d0043d83f46?context=explore][Dockerfile]] is published in Dockerhub). In the process, it sets up ~KAFKA_CFG_BROKER_ID~ with the ~MY_POD_NAME~ environment variable. ~KAFKA_CFG_BROKER_ID~ is used to setup broker id inside the docker image. This way we achieved consistent broker id during restart/rebuild. They are ALWAYS [0,1,2]. After this fix, the metadata of ~__consumer_offsets~ topic is:

#+name: consumer_offsets_metadata_new
#+begin_src shell
Topic: __consumer_offsets	PartitionCount: 50	ReplicationFactor: 3
Configs: compression.type=producer,cleanup.policy=compact,flush.ms=1000,
segment.bytes=104857600,flush.messages=10000,max.message.bytes=1000012,retention.bytes=1073741824
	Topic: __consumer_offsets	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
	Topic: __consumer_offsets	Partition: 1	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1
	Topic: __consumer_offsets	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
	Topic: __consumer_offsets	Partition: 3	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2
	Topic: __consumer_offsets	Partition: 4	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0
	Topic: __consumer_offsets	Partition: 5	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1
	Topic: __consumer_offsets	Partition: 6	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
	Topic: __consumer_offsets	Partition: 7	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1
	Topic: __consumer_offsets	Partition: 8	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
	Topic: __consumer_offsets	Partition: 9	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2
	Topic: __consumer_offsets	Partition: 10	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0
...
#+end_src

That is all for how to achieve consistent broker ids after restart or host rebuild. The key is to tie the ~broker.id~ to ~POD_NAME~. This note is published in [[https://github.com/ghShu/kafka-notes/blob/master/notes/20200310-achieve-consistent-broker-ids-in-kafka-cluster.md][this repository]], which also will be updated with detailed notes on Kafka knowledge summary and deployment details. It provides detailed steps to [[https://github.com/ghShu/kafka-notes/tree/master/deployment][deploy a Kafka cluster to Minikube]] if you are interested to play around.
